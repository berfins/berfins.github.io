<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Evolving Typed Token Processing Networks</title>
  <style>
    body {
      font-family: 'Helvetica Neue', sans-serif;
      background-color: #fef6f9;
      color: #333;
      margin: 0;
      padding: 2rem;
      max-width: 800px;
      margin-left: auto;
      margin-right: auto;
    }
    h1 {
      color: #a64d79;
    }
    h2 {
      color: #b3698e;
      margin-top: 2rem;
    }
    .buttons a {
      display: inline-block;
      margin-right: 1rem;
      margin-top: 1rem;
      padding: 0.5rem 1rem;
      border-radius: 8px;
      background-color: #a64d79;
      color: white;
      text-decoration: none;
      font-size: 0.9rem;
    }
    p {
      font-size: 1rem;
      line-height: 1.6;
    }
  </style>
</head>
<body>
  <h1>Geometric Semantic GP with Linear Scaling: Darwinian versus Lamarckian Evolution


</h1>
  <h2>Authors</h2>
  <p>G. Nadizar, B. Sakallioglu, F. Garrow, S. Silva, L. Vanneschi

</p>
  <h2>Type</h2>
  <p>Journal Article, Genetic Programming and Evolvable Machines (GPEM), 2024

</p>
  <h2>Abstract</h2>
  <p>
Geometric Semantic Genetic Programming (GSGP) has shown notable success in symbolic regression with the introduction of Linear Scaling (LS). This achievement stems from the synergy of the geometric semantic genetic operators of GSGP with the scaling of the individuals for computing their fitness, which favours programs with a promising behaviour. However, the initial combination of GSGP and LS (GSGP-LS) underutilised the potential of LS, scaling individuals only for fitness evaluation, neglecting to incorporate improvements into their genetic material. In this paper we propose an advancement, GSGP with Lamarckian LS (GSGP-LLS), wherein we update the individuals in the population with their scaling coefficients in a Lamarckian fashion, i.e., by inheritance of acquired traits. We assess GSGP-LS and GSGP-LLS against standard GSGP for the task of symbolic regression on five hand-tailored benchmarks and six real-life problems. On the former ones, GSGP-LS and GSGP-LLS both consistently improve GSGP, though with no clear global superiority between them. On the real-world problems, instead, GSGP-LLS steadily outperforms GSGP-LS, achieving faster convergence and superior final performance. Notably, even in cases where LS induces overfitting on challenging problems, GSGP-LLS surpasses GSGP-LS, due to its slower and more localised optimisation steps.
<p>

  <div class="buttons">
    <a href="https://drive.google.com/file/d/1V0J_aMcNWy1MgVSznnaGpFo0DmdyOvLg/view" target="_blank">PDF</a>
    <a href="https://link.springer.com/article/10.1007/s10710-024-09488-0", target="_blank">Publisher Version</a>
    <a href="#">Cite</a>
  </div>
</body>
</html>
